#### Scalability

Scalability: an application/system can handle greater loads by adapting

- vertical scalability, increase hardware performance

- horizontal scalability, increase number of hardware

#### High availability

Usually goes hand in hand with horizontal scaling

Means running your application/system in at least 2 AZs

The goal is to survive a AZ failure

Can be passive (eg. RDS Multi AZ)

Can be active (eg. horizontal scaling)

#### Load balancing

Spread load across multiple downstream instances

Expose a single point of access (DNS) to your application

Semelessly handle failures of downstream instance

Do regular health checks to your instances

Provide SSL termination (HTTPS) for your websites

Enforce stickiness with cookies

High availability across zones

Separate public traffic from private traffic

#### Elastic load balancer (ELB)

An Elastic load balancer is a managed load balancer

- AWS guarantees it works

- AWS takes care for maintenanace, updates, hight availability

- AWS provides only a few configuration knobs

Costs more than setting up your own load balancer but much easier

Integrated with many AWS offerings/services

#### Health checks

Enable a load balancer to know if instances it forwards traffic to are available to reply to request

Crucial for load balancers

Health check is done on a port and route (/health is common)

#### Types of load balancers on AWS

- Classic load balancer (v1 - old generation)

- Application load balancer(v2 - new generation)

- Network load balancer (v2 - new generation)

- Gateway load balancer

It is recommended to use the newer generation load balancers are they provide more features

#### Load balancer security groups

Load balancer can be accessed from anywhere

Application can setup security group to only allow access from load balancer

#### Classic load balancers

Supports TCP (layer 4), HTTP & HTTPS (layer 7)

Health checks are TCP or HTTP based

Fixed hostname (xxx.region.elb.amazonaws.com)

#### Application load balancer

Layer 7 (HTTP) only

Load balancing to multiple HTTP applications across machines (target groups)

Load balancing to multiple applications on the same machine (eg. containers)

Support HTTP/2 and websocket

Support redirects (eg. http to https)

Routing tables to different target gruops:

- routing based on path in url

- routing based on hostname in url

- routing based on query string, headers

Great fit for micro services & container-based application

Has port mapping feature to redirect to a dynamic port in ECS

In comparison, we'd need multiple classic load balancer per application

Target groups can be:

- EC2 instances (HTTP)

- ECS tasks (HTTP)

- Lambda functions (HTTP to JSON event)

- IP addresses (must be private ip)

Can route to multiple target groups

Health checks are at the target group level

Fixed hostname (xxx.region.elb.amazonaws.com)

The application servers don't see the ip of the client directly

- the true ip of the client is inserted in the header X-Forwared-For

- we can also get port (X-Forwarded-Port) and proto (X-Forwared-Proto)

#### Network load balancer

Layer 4, allows to forward TCP & UDP traffic to your instances

Handle millions of request per seconds

Less latency ~100ms (vs 400ms for ALB)

One static ip per AZ, and supports assigning Elastic IP

Target groups can be

- EC2 instances

- IP addresses (private)

- Application load balancer

Health checks support TCP, HTTP, HTTPS protocols

#### Gateway load balancer

Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS

eg. Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, payload manipulation

Operates at layer 3 (network layer) - ip packets

Combines the following functions

- Transparent Network Gateway - single entry/exit for all traffic

- Load balancing - distributes traffic to your virtual appliances

Uses the GENEVE protocol on port 6081

Target groups:

- EC2 instances

- IP addresses (private)

#### Sticky sessions (session affinity)

It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer

Works for classic load balancer & application load balancer

The "cookie" used for stickiness has an expiration date you control

Use case: make sure the user doesn't lose his session data

Enabling stickiness may bring imbalance to the load over the backend EC2 instances

#### Sticky sessions - cookie names

Application-based cookies

- custom cookie
  
  - generated by the target
  
  - can include any custom attributes required by the application
  
  - cookie name must be specified individually for each target group
  
  - dont use AWSALB, AWSALBAPP, or AWSALBTG (reserved)

- Application cookie
  
  - Generated by the load balancer
  
  - cookie name is AWSALBAPP

Duration-based cookie

- cookie generated by the load balancer

- cookie name is AWSALB for ALB, AWSELB for CLB

#### Cross-zone load balancing

Each load balancer instance distributes evenly across all registered instances in all AZ

Application load balancer

- Always on (cant be disabled)

- No charges for inter AZ data

Network load balancer

- Disabled by default

- Pay charges for inter AZ data if enabled

Classic load balancer

- Disabled by default

- No charge for inter AZ data if enabled

#### SSL/TLS

An SSL certificate allow traffic between your clients and your load balancer to be encrypted in transit (in-flight encryption)

SSL: secure sockets layer, used to encrypt connections

TLS: transport layer security, newer version

TLS are mainly used but still refered as SSL

Public SSL certificates are issued by Certificate Authorities (CA)

Comodo, Symantec, GoDaddy, GLobalSIgn, Digicert, etc.

SSL certificates have an expiration date and must be renewed after

User connects to load balancer using https, and load balancer connects to EC2 instance over private VPC http

The load balancer uses an X.509 certificate

You can manage certificates using ACM (AWS certificate manager)

You can create upload your own certificates alternatively

#### Server name indication

SNI solves the problem of loading multiple SSL certificates onto one web server (to server multiple websites)

It's a "newer" protocol and requires the client to indicate the hostname of the target server in the initial SSL handshake

The server will then find the correct certificate or return the default one

Only works for ALB & NLB, CloudFront

Does not work for CLB

#### Elastic load balancers - SSL Certificates

CLB

- only support one SSL certificate

- must use multiple CLB for multiple hostname with multiple SSL certificates

ALB

- supports multiple listeners with multiple SSL certificates

- uses Server Name Indication (SNI) to make it work

NLB

- supports multiple listeners with multiple SSL certificates

#### Connection draining

Feature naming

- Connection draining - for CLB

- Deregistration delay - for ALB & NLB

Time to complete "in-flight requests" while the instance is de-registering or unhealthy

Stops sending new requests to the EC2 instance which is de-registering

Between 1 to 3600 seconds, 0 for disabled

Set to low value if your requests are short

#### Auto scaling groups

Scale out (add EC2 instances) to match an increased load

Scale in (remove EC2 instances) to match a decreased load

Ensure we have a minimum and a maximum number of EC2 instances running

Automatically register new instances to a load balancer

Re-create an EC2 instance in case a previous one is terminated

Free to use (pay for the instances created)

#### Auto scaling group attributes

A Launch Template

- AMI + instance type

- EC2 User Data

- EBS Volumes

- Security Groups

- SSH Key Pair

- IAM Roles for your EC2 instances

- Network + subnets information

- Load balancer information

Min size/max size/initial capacity

Scaling policies

#### Auto scaling - cloudwatch alarms & scaling

it is possible to scale an ASG based on CloudWatch alarms

An alarm monitors a metric (such as Agerage CPU, or a custom metric)

Metrics such as Average CPU are computed for the overall ASG instances

Based on the alarm

- create scale-out policies

- create scale-in policies

#### Auto scaling groups - dynamic scaling policies

Target tracking scaling

- Most simple and easy to setup

- eg. Average ASG CPUT to stay at around 40%

Simple/step scaling

- When a CloudWatch alarm is triggered (eg cpu > 70%) then add 2 units

- When a CloudWatch alarm is triggered (eg cpu < 30%) then remove 1 unit

Scheduled actions

- Anticipate a scaling based on known usage patterns

- eg increase the min capacity to 10 at 5pm on fridays

#### Auto scaling groups - predictive scaling

Continuously forecase load and schedule scaling ahead

#### Good metrics to scale on

CPUUtilization: average cpu utilization across your instances

RequestCountPerTarget: to make sure the number of requests per EC2 instances is stable

Average network in/out: if your application is network bound

Any custom metric: push using CloudWatch

#### Auto scaling groups - scaling cooldowns

After a scaling activity happens, you are in the cooldown period (default 300 seconds)

During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stablize)

Advice: use a ready-to-use AMI to reduce configuration time in order to be serving request faster and reduce the cooldown periods


